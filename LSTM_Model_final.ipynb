{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HanJunSeoJB/security/blob/master/LSTM_Model_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "25Nud7k8GRDU"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "from datetime import timedelta\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import random\n",
        "from itertools import chain\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import LSTM, RepeatVector, TimeDistributed, Dense\n",
        "\n",
        "\n",
        "def dataframe_from_csv(target):\n",
        "    return pd.read_csv(target).rename(columns=lambda x: x.strip())\n",
        "\n",
        "def dataframe_from_csvs(targets):\n",
        "    return pd.concat([dataframe_from_csv(x) for x in targets])\n",
        "\n",
        "TEST_DATASET = sorted([x for x in Path(\"/content/drive/MyDrive/test\").glob(\"*.csv\")])\n",
        "#TRAIN_DATASET = sorted([x for x in Path(\"./test/\").glob(\"*.csv\")])\n",
        "TEST_DF_RAW = dataframe_from_csvs(TEST_DATASET)\n",
        "ATTACK_DF = TEST_DF_RAW['attack']\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TN5bem3nsMQ",
        "outputId": "3efd5224-1f46-436f-a30d-4017b3cb3f34"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9_8xamBnmPb"
      },
      "outputs": [],
      "source": [
        "def extract_sequences_with_context_and_return_list(df):\n",
        "    data = df['attack'].to_numpy().flatten()\n",
        "\n",
        "    flag = 0\n",
        "    start_index = None\n",
        "    start_index_list = []\n",
        "    end_index_list = []\n",
        "    list_size = []\n",
        "\n",
        "    for index, value in enumerate(data):\n",
        "        if value == 1 and flag == 0:\n",
        "            start_index = index\n",
        "            start_index_list.append(start_index)\n",
        "            flag = 1\n",
        "        elif value == 0 and flag == 1:\n",
        "            end_index = index\n",
        "            end_index_list.append(end_index)\n",
        "            sequence_length = end_index - start_index\n",
        "            list_size.append(sequence_length)\n",
        "            print(f\"Sequence length: {sequence_length}\")\n",
        "            flag = 0\n",
        "\n",
        "\n",
        "    if flag == 1:\n",
        "        end_index = len(data)\n",
        "        end_index_list.append(end_index)\n",
        "        sequence_length = end_index - start_index\n",
        "        list_size.append(sequence_length)\n",
        "        print(f\"Sequence length: {sequence_length}\")\n",
        "\n",
        "    return start_index_list, end_index_list, list_size\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jR9t2QERnmPc",
        "outputId": "17896d87-c0c5-44e3-b708-111a7a5b3573"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequence length: 559\n",
            "Sequence length: 309\n",
            "Sequence length: 561\n",
            "Sequence length: 260\n",
            "Sequence length: 358\n",
            "Sequence length: 331\n",
            "Sequence length: 511\n",
            "Sequence length: 255\n",
            "Sequence length: 513\n",
            "Sequence length: 365\n",
            "Sequence length: 179\n",
            "Sequence length: 151\n",
            "Sequence length: 346\n",
            "Sequence length: 513\n",
            "Sequence length: 344\n",
            "Sequence length: 365\n",
            "Sequence length: 630\n",
            "Sequence length: 346\n",
            "Sequence length: 508\n",
            "Sequence length: 567\n",
            "Sequence length: 377\n",
            "Sequence length: 179\n",
            "Sequence length: 564\n",
            "Sequence length: 564\n",
            "Sequence length: 305\n",
            "Sequence length: 513\n",
            "Sequence length: 552\n",
            "Sequence length: 513\n",
            "Sequence length: 386\n",
            "Sequence length: 397\n",
            "Sequence length: 378\n",
            "Sequence length: 290\n",
            "Sequence length: 345\n",
            "Sequence length: 346\n",
            "Sequence length: 2888\n",
            "Sequence length: 335\n",
            "Sequence length: 312\n",
            "Sequence length: 312\n",
            "38 38\n"
          ]
        }
      ],
      "source": [
        "start_list = []\n",
        "end_list = []\n",
        "list_size = []\n",
        "\n",
        "start_list, end_list, list_size = extract_sequences_with_context_and_return_list(TEST_DF_RAW)\n",
        "print(len(start_list), len(end_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2jH09idnmPd"
      },
      "outputs": [],
      "source": [
        "def concateList(df, start_list, end_list, list_size):\n",
        "  segments = []\n",
        "  for start, end, size in zip(start_list, end_list, list_size):\n",
        "        segment = df[start-size:end + 1]\n",
        "        segments.append(segment)\n",
        "\n",
        "  concatenated_df = pd.concat(segments, axis=0, ignore_index=True)\n",
        "  return concatenated_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9-uh4Ig5h_pN"
      },
      "outputs": [],
      "source": [
        "DROP_FIELD = [\",time\", \"attack_P1\", \"attack_P2\", \"attack_P3\",\"attack\"]\n",
        "VALID_COLUMNS_IN_TRAIN_DATASET = TEST_DF_RAW.columns.drop(DROP_FIELD) # DROP_FIELD를 통해 normalization에 사용하지 않을 변수를 제거함.\n",
        "TAG_MIN = TEST_DF_RAW[VALID_COLUMNS_IN_TRAIN_DATASET].min()\n",
        "TAG_MAX = TEST_DF_RAW[VALID_COLUMNS_IN_TRAIN_DATASET].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tLkuccP_nybq"
      },
      "outputs": [],
      "source": [
        "def normalize(df, TAG_MIN, TAG_MAX):\n",
        "    ndf = df.copy()\n",
        "    for c in df.columns:\n",
        "        if TAG_MIN[c] == TAG_MAX[c]:\n",
        "            ndf[c] = df[c] - TAG_MIN[c]\n",
        "        else:\n",
        "            ndf[c] = (df[c] - TAG_MIN[c]) / (TAG_MAX[c] - TAG_MIN[c])\n",
        "    return ndf\n",
        "\n",
        "# Min-Max Normalize\n",
        "TEST_DF = normalize(TEST_DF_RAW[VALID_COLUMNS_IN_TRAIN_DATASET], TAG_MIN, TAG_MAX).ewm(alpha=0.9).mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zwbw6OC3orGo",
        "outputId": "71b288d0-7091-42be-e5cd-23347b026441"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(False, False, False, 0         False\n",
            "1         False\n",
            "2         False\n",
            "3         False\n",
            "4         False\n",
            "          ...  \n",
            "152995    False\n",
            "152996    False\n",
            "152997    False\n",
            "152998    False\n",
            "152999    False\n",
            "Length: 444600, dtype: bool)\n"
          ]
        }
      ],
      "source": [
        "def boundary_check(df):\n",
        "    x = np.array(df, dtype=np.float32)\n",
        "    is_string = df.applymap(lambda x: isinstance(x, str))\n",
        "    return np.any(x > 1.0), np.any(x < 0), np.any(np.isnan(x)), np.any(is_string, axis=1)\n",
        "\n",
        "# Boundary Check\n",
        "print(boundary_check(TEST_DF))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcdJZAk5nmPe"
      },
      "outputs": [],
      "source": [
        "#준지도학습\n",
        "\n",
        "window_size = 60\n",
        "label_size = 30000\n",
        "def sliding_window2(df):\n",
        "    features = []\n",
        "    targets = []\n",
        "    for i in range(0, len(df) - window_size + 1):\n",
        "        # 현재 슬라이스를 직접 리스트로 변환합니다.\n",
        "        window = df[i:i+window_size]\n",
        "        target = 1 if np.any(window['attack'] == 1) else 0\n",
        "        feature = window[['P1_B2004', 'P1_B2016', 'P1_B3004', 'P1_B3005', 'P1_B4002', 'P1_B4005', 'P1_B400B',\n",
        "        'P1_B4022', 'P1_FCV01D', 'P1_FCV01Z', 'P1_FCV02D', 'P1_FCV02Z', 'P1_FCV03D',\n",
        "        'P1_FCV03Z', 'P1_FT01', 'P1_FT01Z', 'P1_FT02', 'P1_FT02Z', 'P1_FT03', 'P1_FT03Z',\n",
        "        'P1_LCV01D', 'P1_LIT01', 'P1_PCV01D', 'P1_PCV01Z', 'P1_PCV02D', 'P1_PCV02Z',\n",
        "        'P1_PIT01', 'P1_PIT02', 'P1_TIT01', 'P1_TIT02']].values\n",
        "        features.append(feature)\n",
        "        targets.append(target)\n",
        "    features = np.array(features, dtype=np.float32)\n",
        "    targets = np.array(targets)\n",
        "    return features, targets\n",
        "\n",
        "features = []\n",
        "targets = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0BCqCV4nmPf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pr_aFGHfnmPf",
        "outputId": "1736ebcb-4d39-4636-e1e1-31fcf56e1afd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(35033, 60, 30)\n",
            "(35033,)\n"
          ]
        }
      ],
      "source": [
        "df = concateList(TEST_DF_RAW, start_list, end_list, list_size)\n",
        "features, targets = sliding_window2(df)\n",
        "print(features.shape)\n",
        "print(targets.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "gEnO6ZOKnmPf"
      },
      "outputs": [],
      "source": [
        "#비지도 학습\n",
        "\n",
        "window_size = 60\n",
        "label_size = 100000\n",
        "def sliding_window_unsupervised(df, window_size, feature_columns, answer_column):\n",
        "    # 데이터 프레임에서 필요한 컬럼만 먼저 추출하여 NumPy 배열로 변환\n",
        "    data = df[feature_columns].values\n",
        "    answers = df.values\n",
        "\n",
        "    num_samples = len(df) - window_size\n",
        "    features = np.empty((num_samples, window_size, len(feature_columns)), dtype=np.float32)\n",
        "    targets = np.empty((num_samples, len(feature_columns)), dtype=np.float32)\n",
        "    answer_targets = np.empty(num_samples, dtype=int)\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        features[i] = data[i:i+window_size]\n",
        "        targets[i] = data[i+window_size]\n",
        "        answer_targets[i] = 1 if np.any(answers[i:i+window_size] == 1) else 0\n",
        "\n",
        "    return features, targets, answer_targets\n",
        "\n",
        "features = []\n",
        "targets = []\n",
        "answers = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhhA9CyUnmPg",
        "outputId": "69de6760-5328-4069-86b2-078870e54154"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(99940, 60, 30)\n",
            "(99940, 30)\n",
            "(99940,)\n"
          ]
        }
      ],
      "source": [
        "feature_columns = ['P1_B2004', 'P1_B2016', 'P1_B3004', 'P1_B3005', 'P1_B4002', 'P1_B4005', 'P1_B400B',\n",
        "                   'P1_B4022', 'P1_FCV01D', 'P1_FCV01Z', 'P1_FCV02D', 'P1_FCV02Z', 'P1_FCV03D',\n",
        "                   'P1_FCV03Z', 'P1_FT01', 'P1_FT01Z', 'P1_FT02', 'P1_FT02Z', 'P1_FT03', 'P1_FT03Z',\n",
        "                   'P1_LCV01D', 'P1_LIT01', 'P1_PCV01D', 'P1_PCV01Z', 'P1_PCV02D', 'P1_PCV02Z',\n",
        "                   'P1_PIT01', 'P1_PIT02', 'P1_TIT01', 'P1_TIT02']\n",
        "\n",
        "features, targets, answers = sliding_window_unsupervised(TEST_DF_RAW[:label_size], 60, feature_columns, ATTACK_DF[:label_size])\n",
        "print(features.shape)\n",
        "print(targets.shape)\n",
        "print(answers.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YoboirBnmPg"
      },
      "outputs": [],
      "source": [
        "features, targets = sliding_window2(TEST_DF_RAW[:label_size])\n",
        "print(features.shape)\n",
        "print(targets.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1alYa62HnmPg"
      },
      "outputs": [],
      "source": [
        "# 데이터 분할\n",
        "features_train, features_test, _, labels_test = train_test_split(features, answers, test_size=0.2,random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_autoencoder(window_size, num_features):\n",
        "    model = Sequential()\n",
        "    # 인코더\n",
        "    model.add(LSTM(100, activation='relu', input_shape=(window_size, num_features), return_sequences=False))\n",
        "    model.add(Dense(num_features))  # 잠재 공간 표현\n",
        "    model.add(RepeatVector(window_size))  # 디코더로 전달할 시퀀스 재생성\n",
        "\n",
        "    # 디코더\n",
        "    model.add(LSTM(100, activation='relu', return_sequences=True))\n",
        "    model.add(TimeDistributed(Dense(num_features)))\n",
        "\n",
        "    # 옵티마이저에 학습률 설정\n",
        "    optimizer = Adam(learning_rate=0.001)\n",
        "    model.compile(optimizer=optimizer, loss='mae')\n",
        "    return model"
      ],
      "metadata": {
        "id": "hh-RsSUNpKHL"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 생성\n",
        "autoencoder = build_autoencoder(window_size, len(feature_columns))"
      ],
      "metadata": {
        "id": "k273eFINpMYi"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 훈련\n",
        "autoencoder.fit(features_train, features_train, epochs=50, batch_size=64, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "t-Av0trbq3q8",
        "outputId": "e06a9002-cd29-4a12-ebe6-a28c21a7a73f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1250/1250 [==============================] - 247s 179ms/step - loss: nan\n",
            "Epoch 2/50\n",
            "1002/1250 [=======================>......] - ETA: 43s - loss: nan"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-4de7b66bfa43>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 모델 훈련\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvlgMZ3wnmPg"
      },
      "outputs": [],
      "source": [
        "# 모델 정의\n",
        "model = Sequential([\n",
        "    LSTM(50, input_shape=(60, 30), return_sequences=True),  # 첫 번째 LSTM 층\n",
        "    Dropout(0.2),  # 과적합 방지를 위한 드롭아웃\n",
        "    LSTM(50),  # 두 번째 LSTM 층\n",
        "    Dropout(0.2),  # 과적합 방지를 위한 드롭아웃\n",
        "    Dense(1, activation='sigmoid')  # 출력 층, 이진 분류를 위한 시그모이드 활성화 함수\n",
        "])\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KjUNBKfnmPg",
        "outputId": "7d7d6059-ec59-475c-dea1-e380628ad08b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train on 28026 samples, validate on 7007 samples\n",
            "Epoch 1/10\n",
            "28026/28026 [==============================] - 47s 2ms/sample - loss: 0.5332 - acc: 0.7030 - val_loss: 0.4949 - val_acc: 0.7237\n",
            "Epoch 2/10\n",
            "28026/28026 [==============================] - 44s 2ms/sample - loss: 0.4945 - acc: 0.7299 - val_loss: 0.5028 - val_acc: 0.7159\n",
            "Epoch 3/10\n",
            "28026/28026 [==============================] - 43s 2ms/sample - loss: 0.5523 - acc: 0.6850 - val_loss: 0.5506 - val_acc: 0.6897\n",
            "Epoch 4/10\n",
            "28026/28026 [==============================] - 43s 2ms/sample - loss: 0.5319 - acc: 0.6913 - val_loss: 0.5492 - val_acc: 0.6925\n",
            "Epoch 5/10\n",
            "28026/28026 [==============================] - 43s 2ms/sample - loss: 0.5414 - acc: 0.6825 - val_loss: 0.5209 - val_acc: 0.6973\n",
            "Epoch 6/10\n",
            "28026/28026 [==============================] - 44s 2ms/sample - loss: 0.5189 - acc: 0.7037 - val_loss: 0.5151 - val_acc: 0.7067\n",
            "Epoch 7/10\n",
            "28026/28026 [==============================] - 44s 2ms/sample - loss: 0.5149 - acc: 0.7085 - val_loss: 0.5097 - val_acc: 0.7071\n",
            "Epoch 8/10\n",
            "28026/28026 [==============================] - 44s 2ms/sample - loss: 0.4997 - acc: 0.7249 - val_loss: 0.6628 - val_acc: 0.6866\n",
            "Epoch 9/10\n",
            "28026/28026 [==============================] - 43s 2ms/sample - loss: 0.4984 - acc: 0.7306 - val_loss: 0.4848 - val_acc: 0.7397\n",
            "Epoch 10/10\n",
            "28026/28026 [==============================] - 43s 2ms/sample - loss: 0.5183 - acc: 0.7124 - val_loss: 0.5334 - val_acc: 0.7034\n"
          ]
        }
      ],
      "source": [
        "# EarlyStopping 설정\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
        "# 모델 학습\n",
        "history = model.fit(features_train, labels_train, epochs=10, batch_size=64, validation_data=(features_test, labels_test), verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-2XtToanmPh"
      },
      "outputs": [],
      "source": [
        "# 예측 수행\n",
        "predictions = model.predict(features_test)\n",
        "predictions = (predictions > 0.5).astype(int)\n",
        "\n",
        "# 정확도 계산\n",
        "acc = accuracy_score(labels_test, predictions)\n",
        "print(\"Accuracy:\", acc)\n",
        "\n",
        "# F1 스코어 계산\n",
        "f1 = f1_score(labels_test, predictions)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "# 혼동 행렬 계산\n",
        "cm = confusion_matrix(labels_test, predictions)\n",
        "print(\"Confusion Matrix:\\n\", cm)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델로부터 예측값을 얻습니다.\n",
        "predicted = autoencoder.predict(X_test)\n",
        "\n",
        "# 각 피쳐별 이상 여부 판단 (임계값은 예를 들어 0.01로 가정)\n",
        "threshold = 0.01\n",
        "anomalies = np.abs(predicted - X_test) > threshold\n",
        "\n",
        "# 이상 여부 판단 결과를 이진 플래그로 변환 (피처 중 하나라도 이상이면 이상으로 판단)\n",
        "anomaly_flags = np.any(anomalies, axis=1).astype(int)\n",
        "\n",
        "# 실제 공격 레이블과의 비교\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(labels_test, anomaly_flags))\n",
        "\n",
        "print(\"Accuracy Score:\")\n",
        "print(accuracy_score(labels_test, anomaly_flags))\n",
        "\n",
        "print(\"F1 Score:\")\n",
        "print(f1_score(labels_test, anomaly_flags, average='macro'))  # 'macro', 'micro', 'weighted' 등 필요에 따라 조정\n",
        "\n",
        "# 컨퓨전 매트릭스 생성 및 시각화\n",
        "cm = confusion_matrix(labels_test, anomaly_flags)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Anomaly'], yticklabels=['Normal', 'Anomaly'])\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8Lr2_a3DwIAr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}